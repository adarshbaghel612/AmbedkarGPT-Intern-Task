# ğŸ” RAG (Retrieval-Augmented Generation) with LangChain 0.3+, Ollama & FAISS

This project implements a modern, fully local Retrieval-Augmented Generation (RAG)
pipeline using:

- LangChain 0.3+ (LCEL-based RAG)
- Ollama as the local LLM backend
- FAISS vector store for fast semantic search
- Local TXT/PDF documents

The goal is to allow users to chat with their documents offline, privately,
and efficiently.

---

## ğŸš€ Features

- Latest LangChain RAG pipeline using LCEL
- Local embeddings generated by Ollama (`qwen2:0.5b`)
- FAISS vectorstore for high-speed document retrieval
- Plug-and-play Jupyter Notebook
- Streamlit chatbot UI
- Supports TXT & PDF documents
- Private and offline â€” No external API calls

---
## ğŸ§  RAG Workflow

1. Load document  
2. Split into chunks  
3. Embed chunks using Ollama  
4. Store embeddings in FAISS  
5. Retrieve top chunks for each question  
6. Pass context into LCEL RAG pipeline  
7. LLM generates accurate answer  

## ğŸ‘¤ Author

Adarsh Baghel  
https://github.com/adarshbaghel612
adarshreigns76626@gmail.com

